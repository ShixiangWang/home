---
title: "《机器学习》学习笔记"
author: "王诗翔"
date: "2020-09-21"
lastmod: "2020-09-21"
slug: ""
# All available categories:
# bioinformatics, config, docker, golang, life, linux, ml, r, read, shell, thinking
categories: ["ml"]
tags: ["machine-learning", "math", "统计"]
---

## 模型评估与选择

### 评估方法

#### 留出法（hold-out）

将数据集划分为两个互斥的集合，其中一个作为训练集，另一个作为测试集。
在训练集上训练出模型后，使用测试集评估测试误差，作为对泛化误差的估计。

需要注意，训练/测试集划分要尽量保持数据分布的一致性。
分类任务中只要要保证样本的类别比例相似，可以使用“分层采样”的办法进行划分。

常见的做法是大约 2/3～4/5 的样本用作训练。

#### 交叉验证法（cross validation）

将数据集划分为 k 个大小相似的互斥子集。每个子集尽可能保持数据分布的一致性。
然后每次用 k-1 个子集的并集作为训练集，余下的一个子集作为测试集。
最终返回的是 k 个测试结果的平均值。
显然，结果的稳定性和保真很大程度取决于 k 的取值，一般取 10。

若令 k 等于样本数，则得到一个特例：留一法（Leave-One-Out，LOO）。

留一法的评估结果往往比较准确，但不适用于大的数据集（计算开销大）。


#### 自助法（bootstrapping）

留出法和交叉验证由于没有使用到一些样本可能会引入估计偏差，而留一法计算复杂度又高。
比较好的解决方案是“自助法”（bootstrapping），它以自助采样（bootstrap sampling）为基础。
即如果一个数据集有 m 个样本，我们对它随机重采样 m 个样本（一个样本可能多次被提取）。
由于数据集样本在采样数据集样本中可能多次出现，因此没有被采样的样本可以作为测试集。

我们可以估计样本在 m 次采样中不被采到的概率为 `$(1 - \frac{1}{m})^m$`，极限约为 0.368。

这样我们训练集和测试集都有 m 个样本。这样的结果也称为“包外估计”（out-of-bag estimate）。

自助法在数据集较少、难以有效划分训练/测试集时很有用。

不过，自助法产生的数据集改变了初始数据集的分布，可能会引入估计偏差。

#### 调参

除了要对适用学习算法进行选择，还需要对算法参数进行设定，就是所谓的调参（parameter tuning）。

#### 最终模型

在确定学习算法和参数以后，此时应该用全部样本对模型训练得到最终的生产模型。


### 性能度量

#### 回归任务

最常用的是均方误差。

#### 分类任务

- 错误率 error rate
- 精度 accuracy
- 准确度 precision
- 查全率、召回率 recall
- 常用的是 F1，泛化后是 F beta
- 真阳性率
- 假阳性率
- ROC/AUC
- ROC 隐性假设了均等代价，不均等时使用代价曲线作为评估手段

曲线：

- P-R 曲线：平衡点是准确率与召回率相等时取值

#### 假设检验

- 二项检验
- t 检验：检验多次留出法或交叉验证的平均错误率
- McNemar 检验：生成两个算法针对分类结果的混淆矩阵，然后进行检验
- Friedman 和 Nemenyi 后续检验：针对多个算法效果进行比较


### 偏差与方差

“偏差-方差分解”（bias-variance decomposition）是解释学习算法泛化性能的一种重要工具。

泛化误差可以分解为偏差、方差和噪声之和。

- 偏差：度量学习算法的期望预测与真实结果的偏离程度，即学习算法的拟合能力。
- 方差：度量同样大小的训练集的变动所导致的学习性能变化，即刻画数据扰动所造成的影星。
- 噪声：表达了当前任务上任何学习算法所能达到的期望泛化误差的下界，即刻画了学习问题本身的难度。

也就是说，**泛化性能是由学习算法的能力，数据的充分性以及学习任务本身的难度所共同决定的**。

一般，偏差和方差存在冲突。



